{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1646032597863,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "IHe_Xgelsho3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1646032599496,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "lfIQ3uTxsk_e",
    "outputId": "d95b5294-c445-479a-de4f-a112c64de8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "Number of examples: 60000\n",
      "Shape of the images in the dataset: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "# fashion_mnist = keras.datasets.fashion_mnist\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(f\"Number of examples: {len(train_images)}\")\n",
    "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
    "\n",
    "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
    "train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "all_digits = train_images\n",
    "all_labels = train_labels\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "#all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)\n",
    "\n",
    "train_images = all_digits\n",
    "train_labels = all_labels\n",
    "\n",
    "generator_in_channels = noise_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1646032602957,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "N8ihJdAnc2cc",
    "outputId": "05a50417-5215-4d95-9650-0a4887f1e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 28, 28, 11)]      0         \n",
      "                                                                 \n",
      " zero_padding2d_4 (ZeroPaddi  (None, 32, 32, 11)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 16, 16, 64)        17664     \n",
      "                                                                 \n",
      " leaky_re_lu_28 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 8, 8, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_29 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 4, 4, 256)         819456    \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 2, 2, 512)         3277312   \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,321,409\n",
      "Trainable params: 4,321,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=(28, 28, discriminator_in_channels))\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1646032604916,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "Q1iLiMLpc8TI",
    "outputId": "7d19b47d-d0d3-48da-8fa6-888309c55059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 138)]             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4096)              565248    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 4096)             16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 4096)              0         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 8, 8, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 8, 8, 128)         294912    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 16, 16, 64)        73728     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 32, 32, 1)         576       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 32, 32, 1)        4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " cropping2d_4 (Cropping2D)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 951,620\n",
      "Trainable params: 943,042\n",
      "Non-trainable params: 8,578\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim + num_classes,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "    x = layers.Cropping2D((2, 2))(x)\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1646032606093,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "XW29HFmBdx5K"
   },
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # if isinstance(real_images, tuple):\n",
    "        #     real_images = real_images[0]\n",
    "\n",
    "        #print(real_images)\n",
    "        real_images, one_hot_labels = real_images\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            # random_latent_vectors = tf.random.normal(\n",
    "            #     shape=(batch_size, self.latent_dim)\n",
    "            # )\n",
    "\n",
    "            #########################################################################\n",
    "            # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "            # the images. This is for the discriminator.\n",
    "            image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "            image_one_hot_labels = tf.repeat(\n",
    "                image_one_hot_labels, repeats=[image_size * image_size]\n",
    "            )\n",
    "            image_one_hot_labels = tf.reshape(\n",
    "                image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "            )\n",
    "\n",
    "            # Sample random points in the latent space and concatenate the labels.\n",
    "            # This is for the generator.\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            random_vector_labels = tf.concat(\n",
    "                [random_latent_vectors, one_hot_labels], axis=1\n",
    "            )\n",
    "\n",
    "            # Decode the noise (guided by labels) to fake images.\n",
    "            generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "            # Combine them with real images. Note that we are concatenating the labels\n",
    "            # with these images here.\n",
    "            fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "            real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "            # combined_images = tf.concat(\n",
    "            #     [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "            # )\n",
    "\n",
    "            # Assemble labels discriminating real from fake images.\n",
    "            # labels = tf.concat(\n",
    "            #     [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "            # )\n",
    "            #########################################################################\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                #fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_image_and_labels, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_image_and_labels, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_image_and_labels, fake_image_and_labels)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        #random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        ###########################################################################################\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        #print('random_latent_vectors.shape', random_latent_vectors.shape)\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "        ###########################################################################################\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            #generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            generated_images = self.generator(random_vector_labels, training=True)\n",
    "\n",
    "            generated_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_image_and_labels, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(misleading_labels, gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1646032607277,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "v18pc5tGd3fA"
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        one_hot_labels = tf.reshape(tf.convert_to_tensor([self.num_img * [[1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]]]), (self.num_img, num_classes))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "        generated_images = self.model.generator(random_vector_labels)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.preprocessing.image.array_to_img(img)\n",
    "            img.save(\"gan_training/generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1466923,
     "status": "ok",
     "timestamp": 1646034075101,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "qlFeUJ34d74k",
    "outputId": "15a15019-bfce-4fac-d4a3-86965e875933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 77s 609ms/step - d_loss: -31.7627 - g_loss: -27.9136\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -28.1707 - g_loss: -25.7939\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -19.8019 - g_loss: -20.1300\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -14.3714 - g_loss: -10.8244\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -11.4424 - g_loss: -6.4701\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -14.2109 - g_loss: -5.4267\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -13.7582 - g_loss: -0.1789\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -23.3880 - g_loss: -0.1615\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -37.1342 - g_loss: -3.2976\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -27.5937 - g_loss: -6.8319\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -11.4328 - g_loss: -1.7342\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -6.9005 - g_loss: -2.3488\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -6.6896 - g_loss: -6.1843\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -6.1930 - g_loss: -16.0753\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -5.1173 - g_loss: -19.1337\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -4.5896 - g_loss: -43.5682\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -5.2231 - g_loss: -27.1734\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -4.5040 - g_loss: -60.8431\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -1.9241 - g_loss: -73.6181\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 72s 609ms/step - d_loss: -2.9645 - g_loss: -24.9030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08561ed8d0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "# def generator_loss(fake_img):\n",
    "#     #return -tf.reduce_mean(fake_img)\n",
    "#     return -keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(misleading_labels, predictions):\n",
    "    #return -tf.reduce_mean(fake_img)\n",
    "    return -keras.losses.BinaryCrossentropy(from_logits=True)(misleading_labels, predictions)\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training the model.\n",
    "wgan.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])\n",
    "#wgan.fit(dataset, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1646034533652,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "-X1Dby0SeF_q",
    "outputId": "8ffcc9c6-e92e-483d-9af4-fe0e0fc2c238"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x280 at 0x7F07506BC350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "num_example_images = 1\n",
    "\n",
    "num_latent_dim = 128\n",
    "\n",
    "random_latent_vectors = tf.random.normal(shape=(num_example_images, num_latent_dim))\n",
    "one_hot_labels = tf.reshape(tf.convert_to_tensor(\n",
    "    [num_example_images * [[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]]]), #Create synthetic '6'\n",
    "                            (num_example_images, num_classes))\n",
    "random_vector_labels = tf.concat(\n",
    "    [random_latent_vectors, one_hot_labels], axis=1\n",
    ")\n",
    "generated_images = wgan.generator(random_vector_labels)\n",
    "generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "img = generated_images[0].numpy()\n",
    "img = keras.preprocessing.image.array_to_img(img)\n",
    "\n",
    "# Save file for display\n",
    "img.save(\"example.png\")\n",
    "\n",
    "image = Image.open(\"example.png\")\n",
    "scale = 10\n",
    "display(image.resize(( int(image.width * scale), int(image.height * scale))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1646032407779,
     "user": {
      "displayName": "Vadim Avkhimenia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeHNI2V5t3vJ7LlltSjmj52fXoLlAjMIcQRAh-8g=s64",
      "userId": "02351462041501850590"
     },
     "user_tz": 420
    },
    "id": "0_g-2c8r9LgL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPLtipmM6yoHyJ7xcrU0vBJ",
   "collapsed_sections": [],
   "name": "cwgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
